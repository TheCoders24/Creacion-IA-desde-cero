隆Perfecto! Parece que est谩s listo para continuar. Ahora, vamos a profundizar un poco m谩s en el **descenso del gradiente** y c贸mo se usa para entrenar modelos, tal como se mencion贸 en la tarea.

---

## ** Clase 3: Descenso del Gradiente y Optimizaci贸n de Modelos**

### **1. 驴Qu茅 es el Descenso del Gradiente?**
El **descenso del gradiente** es un algoritmo de optimizaci贸n utilizado para minimizar la funci贸n de **p茅rdida** (error) en los modelos de IA. En otras palabras, nos ayuda a ajustar los par谩metros del modelo (como los pesos en una red neuronal) para que las predicciones sean lo m谩s precisas posibles.

 **Conceptos clave**:
- **Funci贸n de p茅rdida (o costo)**: Mide el error entre la predicci贸n del modelo y los valores reales. Por ejemplo, en regresi贸n, podr铆a ser el **error cuadr谩tico medio** (MSE).
- **Gradiente**: El gradiente es el vector de derivadas parciales de la funci贸n de p茅rdida respecto a los par谩metros del modelo. Nos indica la **direcci贸n** y **magnitud** en la que debemos movernos para reducir el error.
- **Tasa de aprendizaje (Learning Rate)**: Controla el tama帽o de los pasos que damos para ajustar los par谩metros del modelo. Si es demasiado grande, podemos "pasarnos" del m铆nimo; si es demasiado peque帽a, el modelo puede tardar mucho en converger.

### **2. 驴C贸mo funciona el Descenso del Gradiente?**
La idea b谩sica es movernos en la direcci贸n opuesta al gradiente (el **gradiente negativo**) para minimizar el error. El proceso es repetido hasta que el modelo alcanza el m铆nimo de la funci贸n de p茅rdida, es decir, el modelo tiene el mejor rendimiento posible.

### **F贸rmula b谩sica**:
Para cada par谩metro \( \theta \) del modelo:
\[
\theta = \theta - \eta \times \nabla_\theta J(\theta)
\]
- \( \eta \) es la tasa de aprendizaje.
- \( \nabla_\theta J(\theta) \) es el gradiente de la funci贸n de costo \( J(\theta) \) con respecto al par谩metro \( \theta \).

### **3. Tipos de Descenso del Gradiente**
- **Descenso del Gradiente Estoc谩stico (SGD)**: Actualiza los par谩metros despu茅s de cada ejemplo de entrenamiento.
- **Descenso del Gradiente por Lotes (Batch GD)**: Actualiza los par谩metros despu茅s de procesar todo el conjunto de datos.
- **Descenso del Gradiente Mini-Batch**: Es una combinaci贸n de los dos anteriores, actualiza despu茅s de procesar peque帽os subconjuntos de datos (mini-lotes).

---

### **Ejemplo de Descenso del Gradiente en Regresi贸n Lineal**

1. **Problema**: Queremos ajustar una l铆nea recta a un conjunto de datos utilizando descenso del gradiente. La funci贸n de p茅rdida para la regresi贸n lineal es el error cuadr谩tico medio (MSE).

2. **F贸rmula de la predicci贸n**:
\[
\hat{y} = w \cdot x + b
\]
- \( \hat{y} \) es la predicci贸n del modelo.
- \( w \) es el peso (pendiente de la l铆nea).
- \( b \) es el sesgo (intercepto).

3. **Funci贸n de p茅rdida (MSE)**:
\[
J(w, b) = \frac{1}{m} \sum_{i=1}^{m} (\hat{y_i} - y_i)^2
\]
donde \( m \) es el n煤mero de ejemplos en el conjunto de datos.

4. **Gradientes**:
Para actualizar los par谩metros \( w \) y \( b \), calculamos las derivadas parciales de la funci贸n de p茅rdida con respecto a estos par谩metros:
\[
\frac{\partial J(w, b)}{\partial w}, \frac{\partial J(w, b)}{\partial b}
\]

5. **Actualizaci贸n**:
\[
w = w - \eta \cdot \frac{\partial J(w, b)}{\partial w}
\]
\[
b = b - \eta \cdot \frac{\partial J(w, b)}{\partial b}
\]

---

### **C贸digo de Descenso del Gradiente en Python**:
```python
import numpy as np
import matplotlib.pyplot as plt

# Generamos datos de ejemplo (y = 2x + 1)
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 2 * X + 1 + np.random.randn(100, 1)  # Agregamos algo de ruido

# Inicializamos los par谩metros
w = np.random.randn(1)
b = np.random.randn(1)
learning_rate = 0.01
epochs = 1000
m = len(X)

# Funci贸n de costo (MSE)
def compute_cost(X, y, w, b):
    predictions = X.dot(w) + b
    cost = (1/(2*m)) * np.sum((predictions - y) ** 2)
    return cost

# Descenso del gradiente
for epoch in range(epochs):
    # Predicci贸n
    y_pred = X.dot(w) + b
    
    # C谩lculo de gradientes
    dw = (1/m) * np.dot(X.T, (y_pred - y))
    db = (1/m) * np.sum(y_pred - y)
    
    # Actualizaci贸n de par谩metros
    w -= learning_rate * dw
    b -= learning_rate * db
    
    # Imprimir el costo cada 100 iteraciones
    if epoch % 100 == 0:
        print(f'Epoch {epoch}: Cost {compute_cost(X, y, w, b)}')

# Mostrar la recta ajustada
plt.scatter(X, y)
plt.plot(X, X.dot(w) + b, color='red')
plt.title('Regresi贸n Lineal con Descenso del Gradiente')
plt.xlabel('X')
plt.ylabel('y')
plt.show()
```

---

## ** Tarea para la pr贸xima clase:**
1. **Investiga y comprende la regularizaci贸n** (L1 y L2) en los modelos de IA, ya que ayuda a evitar el sobreajuste (overfitting).
2. **Practica el c贸digo de descenso del gradiente**, experimentando con diferentes tasas de aprendizaje y 茅pocas.

---

 驴Te gustar铆a continuar con la implementaci贸n de otros algoritmos de optimizaci贸n o prefieres profundizar en regularizaci贸n y t茅cnicas de ajuste de modelos? 