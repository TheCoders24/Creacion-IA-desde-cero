    
---

### 1. **Investigar sobre la regularización L1, L2 y ElasticNet y cómo se ajustan en diferentes modelos**

La **regularización** es una técnica fundamental para mejorar la capacidad de generalización de los modelos de IA y evitar el sobreajuste (overfitting). Existen diferentes tipos de regularización:

#### **L1 (Lasso)**
- **¿Qué hace?**: Agrega una penalización a la suma de los valores absolutos de los pesos del modelo.
- **¿Cómo ayuda?**: L1 tiene la propiedad de "eliminar" características, ya que algunos de los pesos pueden volverse exactamente cero, lo que lleva a un modelo más simple y más interpretativo.
- **¿Dónde se usa?**: L1 es útil cuando se cree que solo un pequeño subconjunto de las características es relevante, ya que automáticamente realiza una selección de características.

#### **L2 (Ridge)**
- **¿Qué hace?**: Agrega una penalización a la suma de los cuadrados de los pesos del modelo.
- **¿Cómo ayuda?**: L2 penaliza los pesos grandes, pero no los lleva exactamente a cero. Esto ayuda a evitar que el modelo se enfoque demasiado en características con valores extremos y hace que el modelo sea más general.
- **¿Dónde se usa?**: L2 es útil cuando hay muchas características y se desea evitar que alguna de ellas tenga un peso excesivo, pero no se quiere eliminar ninguna.

#### **ElasticNet**
- **¿Qué hace?**: Combina tanto L1 como L2. La idea es usar lo mejor de ambos mundos: la selección automática de características de L1 y la estabilidad de L2.
- **¿Cómo ayuda?**: ElasticNet es útil cuando hay muchas características correlacionadas entre sí. L1 puede eliminar algunas de estas características, pero L2 mantiene el modelo estable y evita el sobreajuste.
- **¿Dónde se usa?**: Se utiliza cuando se sospecha que tanto la selección de características (L1) como la penalización de grandes pesos (L2) son necesarias.

---

### 2. **Prueba el código de descenso del gradiente con regularización L2, cambiando el valor de \( \lambda \) y observando cómo afecta al modelo**

- **Objetivo**: El valor de \( \lambda \) controla la fuerza de la penalización en la regularización L2.
  - **Si \( \lambda \) es grande**: El modelo se verá más penalizado por los grandes pesos, lo que reducirá la complejidad del modelo y posiblemente lo hará menos preciso.
  - **Si \( \lambda \) es pequeño**: El modelo tendrá más libertad para ajustarse a los datos, lo que puede llevar a un sobreajuste si el valor de \( \lambda \) es muy bajo.
  
- **Tarea práctica**: Ejecutar el código de descenso del gradiente con regularización L2 y variar el valor de \( \lambda \) para ver cómo cambia el modelo. Observa cómo afecta el costo y el comportamiento de la recta ajustada a los datos.

---

### 3. **Compara los resultados entre usar o no regularización, y experimenta con diferentes tasas de aprendizaje y épocas**

- **Comparar resultados con y sin regularización**: Esto te permitirá observar si la regularización mejora la generalización del modelo o si en realidad lo hace menos preciso al intentar forzar los pesos a ser pequeños.
  - **Sin regularización**: El modelo puede ajustarse demasiado a los datos de entrenamiento y tener un desempeño pobre en datos nuevos.
  - **Con regularización**: El modelo debería tener un mejor desempeño general en datos no vistos, pero podría perder algo de precisión en los datos de entrenamiento debido a la penalización.

- **Tasas de aprendizaje**: La tasa de aprendizaje controla cuán rápido ajusta el modelo sus parámetros. Si es demasiado alta, el modelo puede no converger; si es demasiado baja, el entrenamiento será lento. Experimenta con diferentes valores de la tasa de aprendizaje para ver cómo afecta la velocidad de convergencia y la calidad del modelo.

- **Épocas**: Son el número de veces que el modelo pasa por todos los datos de entrenamiento. Si tienes demasiadas épocas, el modelo puede sobreajustarse. Si tienes pocas épocas, el modelo podría no entrenarse lo suficiente. Experimenta con distintas cantidades de épocas para encontrar el punto donde el modelo converge de forma óptima.

---

### **Resumen de lo que debes hacer:**
1. **Investigar** los tres tipos de regularización (L1, L2 y ElasticNet) y cómo se utilizan en distintos modelos.
2. **Probar el código de descenso del gradiente con regularización L2**, modificando \( \lambda \) y observando su efecto en el modelo.
3. **Comparar los resultados** entre usar regularización y no usarla, ajustando también la tasa de aprendizaje y las épocas.

---
